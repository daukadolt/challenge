# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

_file_map = {
    "clients.baml": '// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\n// Using the new OpenAI Responses API for enhanced formatting\nclient<llm> CustomGPT5 {\n  provider openai-responses\n  options {\n    model "gpt-5"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT5Mini {\n  provider openai-responses\n  retry_policy Exponential\n  options {\n    model "gpt-5-mini"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\n// Openai with chat completion\nclient<llm> CustomGPT5Chat {\n  provider openai\n  options {\n    model "gpt-5"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\n// Latest Anthropic Claude 4 models\nclient<llm> CustomOpus4 {\n  provider anthropic\n  options {\n    model "claude-opus-4-1-20250805"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\nclient<llm> CustomSonnet4 {\n  provider anthropic\n  options {\n    model "claude-sonnet-4-20250514"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\nclient<llm> CustomHaiku {\n  provider anthropic\n  retry_policy Constant\n  options {\n    model "claude-3-5-haiku-20241022"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n// Example Google AI client (uncomment to use)\n// client<llm> CustomGemini {\n//   provider google-ai\n//   options {\n//     model "gemini-2.5-pro"\n//     api_key env.GOOGLE_API_KEY\n//   }\n// }\n\n// Example AWS Bedrock client (uncomment to use)\n// client<llm> CustomBedrock {\n//   provider aws-bedrock\n//   options {\n//     model "anthropic.claude-sonnet-4-20250514-v1:0"\n//     region "us-east-1"\n//     // AWS credentials are auto-detected from env vars\n//   }\n// }\n\n// Example Azure OpenAI client (uncomment to use)\n// client<llm> CustomAzure {\n//   provider azure-openai\n//   options {\n//     model "gpt-5"\n//     api_key env.AZURE_OPENAI_API_KEY\n//     base_url "https://MY_RESOURCE_NAME.openai.azure.com/openai/deployments/MY_DEPLOYMENT_ID"\n//     api_version "2024-10-01-preview"\n//   }\n// }\n\n// Example Vertex AI client (uncomment to use)\n// client<llm> CustomVertex {\n//   provider vertex-ai\n//   options {\n//     model "gemini-2.5-pro"\n//     location "us-central1"\n//     // Uses Google Cloud Application Default Credentials\n//   }\n// }\n\n// Example Ollama client for local models (uncomment to use)\n// client<llm> CustomOllama {\n//   provider openai-generic\n//   options {\n//     base_url "http://localhost:11434/v1"\n//     model "llama4"\n//     default_role "user" // Most local models prefer the user role\n//     // No API key needed for local Ollama\n//   }\n// }\n\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\nclient<llm> CustomFast {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [CustomGPT5Mini, CustomHaiku]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [CustomGPT5Mini, CustomGPT5]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}',
    "control.baml": 'enum RuleType {\n    Blocking @description(#"\n        Rules that must pass immediately before a merge or release.\n        Example: "All tests passed", "Coverage > 80%"\n    "#)\n    Audit @description(#"\n        Maintenance or process rules that are measured over time, not per PR.\n        Example: "Flaky tests fixed within 2 days", "Quarterly policy review"\n    "#)\n}\n\nclass Rule {\n    id string @description("Short identifier, e.g. \'TEST-01\'")\n    \n    type RuleType\n    \n    description string\n    \n    logic string @description(#"\n        Propositional Logic. Be explicit with variable values.\n        BAD: "CriticalPaths -> 100% Coverage"\n        GOOD: "FilePath.contains(\'security\', \'payment\') -> Coverage == 100%"\n    "#)\n    \n    exceptions string[] @description(#"\n        List of specific scenarios where this rule can be skipped.\n    "#)\n}\n\nclass Control {\n    name string\n    description string\n    rules Rule[]\n}\n\nenum RuleStatus {\n    PASS\n    FAIL\n    MORE_INFOMATION_NEEDED\n    NA @alias("N/A")\n}\n\nclass RuleEvaluation {\n    rule_id string \n    status RuleStatus\n    reasoning string \n}\nfunction EvaluateCompliance(facts_list: string[], control: Control) -> RuleEvaluation[] {\n    client "openai/gpt-4o"\n    prompt #"\n        You are an AI Auditor evaluating a set of technical evidence for a Pull Request.\n\n        ### The Control Policy\n        {{ control }}\n\n        ### The Evidence Facts\n        Below is the aggregated data extracted from multiple files.\n\n        {% for fact in facts_list %}\n        --- Evidence Item {{ loop.index }} ---\n        {{ fact }}\n        {% endfor %}\n\n        ### Instructions\n        1. Evaluate the COMPOSITE state of the evidence. \n           (e.g. If Evidence 1 proves the PR is \'main\', and Evidence 2 proves coverage is \'80%\', the rule PASSES).\n        2. If a Blocking rule is not satisfied by the combined evidence, mark it FAIL.\n\n        {{ ctx.output_format }}\n    "#\n}\n\nfunction ExtractControl(control: string) -> Control {\n    client "openai-responses/gpt-5-mini" \n    prompt #"\n        Extract the control logic from the following policy document.\n        \n        ### Role\n        You are a Compliance Architect defining rules for an AI Auditor. \n        Your output will be used to verify evidence (screenshots/logs) which may contain system-specific variations (e.g., "microsoft:main" vs "main").\n\n        ### Extraction Guidelines\n        1. **Robust Operators**: \n           - Avoid strict equality (`==`) for names, paths, or identifiers. Real-world data is messy.\n           - Prefer **containment** or **matching** logic (e.g., `contains`, `matches`, `includes`) for text fields.\n           - Use strict equality (`==`) ONLY for booleans (`true`/`false`) or fixed enums (`\'approved\'`, `\'clean\'`).\n\n        2. **Atomic Granularity**: \n           - Break complex policies into the smallest possible testable units.\n           - ONE rule = ONE concept. (e.g., Split "Review by independent peer" into "Review performed" AND "Reviewer is independent").\n\n        3. **Contextual Gating**:\n           - Explicitly define *when* a rule applies using the format: `Context -> Requirement`.\n           - Example: `PR.changes.include_code -> Coverage.exists`\n\n        ### Content to Extract\n        {{ control }}\n\n        {{ ctx.output_format }}\n    "#\n}',
    "evidence.baml": '// -----------------------------------------------------------------------------\n// 1. Shared Input Wrapper (Supports Multimodal)\n// -----------------------------------------------------------------------------\nclass EvidenceInput {\n    img image? @description("A screenshot or image of the evidence")\n    text string? @description("Textual content of the evidence (logs, code, etc.)")\n    // pdf, audio, and video are standard BAML types for multimodal inputs\n    // also, they are reserved keywords ¯\\_(ツ)_/¯\n    pdf_file pdf? @description("A PDF document containing evidence")\n    audio_file audio? @description("Audio recording of evidence")\n    video_file video? @description("Video recording of evidence")\n}\n\n// -----------------------------------------------------------------------------\n// 2. Expanded Enum\n// -----------------------------------------------------------------------------\nenum EvidenceType {\n    PullRequest @description("GitHub/GitLab PR/MR interface showing status, reviewers, or conversation")\n    CoverageReport @description("SonarQube, Codecov, terminal or interface output showing coverage %")\n    CIPipeline @description("CI/CD interface showing build steps, jobs, and pass/fail status")\n    TestResult @description("JUnit/TestNG/Pytest output, or UI showing specific test case pass/fail counts")\n    SecurityScan @description("Snyk, SonarQube Security, or Trivy output showing vulnerability counts")\n    IssueBoard @description("Jira, Linear, or GitHub Issues list showing active bugs/tickets")\n    PerformanceReport @description("JMeter, K6, or Lighthouse report showing latency/throughput metrics")\n    GitCommit @description("Git commit view showing hash, author, message, and signature status")\n    Unknown @description("Content is unclear or unrelated to code/deployment")\n}\n\n// -----------------------------------------------------------------------------\n// 3. Router Function\n// -----------------------------------------------------------------------------\nfunction IdentifyEvidenceType(input: EvidenceInput) -> EvidenceType {\n    client "openai/gpt-4o"\n    prompt #"\n        Identify the type of technical interface, log, or media shown.\n        {{ _.role("user") }}\n        \n        {% if input.text %}\n        Here is the text content:\n        {{ input.text }}\n        {% endif %}\n\n        {% if input.img %}\n        {{ input.img }}\n        {% endif %}\n\n        {% if input.pdf_file %}\n        {{ input.pdf_file }}\n        {% endif %}\n\n        {% if input.audio_file %}\n        {{ input.audio_file }}\n        {% endif %}\n\n        {% if input.video_file %}\n        {{ input.video_file }}\n        {% endif %}\n\n        {{ ctx.output_format }}\n    "#\n}\n\n// -----------------------------------------------------------------------------\n// 4. Extraction Functions\n// -----------------------------------------------------------------------------\n\n// --- Pull Request ---\nclass PRFacts {\n    platform string? @description("The hosting platform (e.g. GitHub, GitLab, BitBucket)")\n    pr_number string? @description("The numerical ID or identifier of the Pull Request")\n    author_username string? @description("Username of the person who opened the PR")\n    reviewer_usernames string[] @description("List of usernames who reviewed the PR")\n    status string? @description("Current state of the PR (Open, Merged, Closed, Draft)")\n    base_branch string? @description("The target branch into which changes are being merged")\n    status_checks_passing bool? @description("Whether all automated status checks (CI, analysis) are passing")\n}\n\nfunction ExtractPRFacts(input: EvidenceInput) -> PRFacts {\n    client "openai/gpt-4o"\n    prompt #"\n        Extract the objective metadata from this Pull Request evidence.\n        Do not infer information that is not explicitly visible/audible.\n\n        {{ _.role("user") }}\n        \n        {% if input.text %}\n        {{ input.text }}\n        {% endif %}\n\n        {% if input.img %}\n        {{ input.img }}\n        {% endif %}\n\n        {% if input.pdf_file %}\n        {{ input.pdf_file }}\n        {% endif %}\n\n        {% if input.audio_file %}\n        {{ input.audio_file }}\n        {% endif %}\n\n        {% if input.video_file %}\n        {{ input.video_file }}\n        {% endif %}\n\n        {{ ctx.output_format }}\n    "#\n}\n\n// --- Git Commit ---\nclass CommitFacts {\n    commit_hash string? @description("The full or abbreviated SHA-1 hash of the commit")\n    author_username string? @description("The username or email of the commit author")\n    commit_message string? @description("The text message describing the commit changes")\n    is_verified_signature bool? @description("Whether the commit has a verified GPG/SSH signature")\n    parent_pr_id string? @description("The ID of the Pull Request this commit belongs to, if visible")\n}\n\nfunction ExtractCommitFacts(input: EvidenceInput) -> CommitFacts {\n    client "openai/gpt-4o"\n    prompt #"\n        Extract metadata from this Git Commit view.\n        {{ _.role("user") }}\n        \n        {% if input.text %}\n        {{ input.text }}\n        {% endif %}\n\n        {% if input.img %}\n        {{ input.img }}\n        {% endif %}\n\n        {% if input.pdf_file %}\n        {{ input.pdf_file }}\n        {% endif %}\n\n        {% if input.audio_file %}\n        {{ input.audio_file }}\n        {% endif %}\n\n        {% if input.video_file %}\n        {{ input.video_file }}\n        {% endif %}\n\n        {{ ctx.output_format }}\n    "#\n}\n\n// --- Code Coverage ---\nclass CoverageFacts {\n    tool_name string? @description("The name of the coverage tool (e.g. JaCoCo, Istanbul, Coverage.py)")\n    overall_line_coverage float? @description("The percentage of total lines covered by tests (0.0 to 100.0)")\n    new_code_line_coverage float? @description("The percentage of new/modified lines covered by tests")\n    branch_coverage float? @description("The percentage of control flow branches executed")\n    function_coverage float? @description("The percentage of functions/methods executed")\n    visible_files_tested string[] @description("List of filenames or paths explicitly shown in the report")\n}\n\nfunction ExtractCoverageFacts(input: EvidenceInput) -> CoverageFacts {\n    client "openai/gpt-4o"\n    prompt #"\n        Extract the test coverage metrics visible in this report.\n        If a specific metric (e.g. Branch Coverage) is not visible, leave it null.\n        \n        {{ _.role("user") }}\n        \n        {% if input.text %}\n        {{ input.text }}\n        {% endif %}\n\n        {% if input.img %}\n        {{ input.img }}\n        {% endif %}\n\n        {% if input.pdf_file %}\n        {{ input.pdf_file }}\n        {% endif %}\n\n        {% if input.audio_file %}\n        {{ input.audio_file }}\n        {% endif %}\n\n        {% if input.video_file %}\n        {{ input.video_file }}\n        {% endif %}\n\n        {{ ctx.output_format }}\n    "#\n}\n\n// --- CI Pipeline ---\nclass CIFacts {\n    pipeline_id string? @description("Unique identifier for the CI pipeline run")\n    overall_status string? @description("The final aggregate status (Success, Failed, Canceled, Running)")\n    // Key: Stage name (e.g. \'Build\', \'Test\'), Value: Status\n    stage_statuses map<string, string> @description("Map of stage names to their individual status (e.g. \'Build\': \'Success\')")\n    environment string? @description("The target deployment environment (e.g. \'clean\', \'prod\', \'staging\')")\n    has_warnings bool @description("Whether there are non-blocking warnings in the pipeline output")\n}\n\nfunction ExtractCIFacts(input: EvidenceInput) -> CIFacts {\n    client "openai/gpt-4o"\n    prompt #"\n        Extract the CI/CD pipeline status. \n        Note: Differentiate between a single job failing vs the whole pipeline failing.\n        \n        {{ _.role("user") }}\n        \n        {% if input.text %}\n        {{ input.text }}\n        {% endif %}\n\n        {% if input.img %}\n        {{ input.img }}\n        {% endif %}\n\n        {% if input.pdf_file %}\n        {{ input.pdf_file }}\n        {% endif %}\n\n        {% if input.audio_file %}\n        {{ input.audio_file }}\n        {% endif %}\n\n        {% if input.video_file %}\n        {{ input.video_file }}\n        {% endif %}\n\n        {{ ctx.output_format }}\n    "#\n}\n\n// --- Test Results ---\nclass TestResultFacts {\n    framework string? @description("The testing framework used (e.g. JUnit, Pytest, Cypress)")\n    total_tests int? @description("The total number of tests executed")\n    passed int? @description("The count of tests that passed")\n    failed int? @description("The count of tests that failed")\n    skipped int? @description("The count of tests that were skipped or pending")\n    duration_seconds float? @description("Total execution time in seconds")\n    failed_test_names string[] @description("Names or identifiers of specific tests that failed")\n}\n\nfunction ExtractTestResultFacts(input: EvidenceInput) -> TestResultFacts {\n    client "openai/gpt-4o"\n    prompt #"\n        Extract testing metrics.\n        \n        {{ _.role("user") }}\n        \n        {% if input.text %}\n        {{ input.text }}\n        {% endif %}\n\n        {% if input.img %}\n        {{ input.img }}\n        {% endif %}\n\n        {% if input.pdf_file %}\n        {{ input.pdf_file }}\n        {% endif %}\n\n        {% if input.audio_file %}\n        {{ input.audio_file }}\n        {% endif %}\n\n        {% if input.video_file %}\n        {{ input.video_file }}\n        {% endif %}\n\n        {{ ctx.output_format }}\n    "#\n}\n\n// --- Security Scan ---\nclass SecurityFacts {\n    scanner_name string? @description("Name of the security scanning tool (e.g. Snyk, Trivy)")\n    severity_counts map<string, int> @description("Map of severity levels (Critical, High, Medium, Low) to issue counts")\n    total_vulnerabilities int? @description("Total count of detected vulnerabilities")\n    top_critical_issues string[] @description("List of the most severe vulnerability names or IDs detected")\n    is_passing bool? @description("Whether the scan meets the configured passing criteria")\n}\n\nfunction ExtractSecurityFacts(input: EvidenceInput) -> SecurityFacts {\n    client "openai/gpt-4o"\n    prompt #"\n        Extract security vulnerability data.\n        \n        {{ _.role("user") }}\n        \n        {% if input.text %}\n        {{ input.text }}\n        {% endif %}\n\n        {% if input.img %}\n        {{ input.img }}\n        {% endif %}\n\n        {% if input.pdf_file %}\n        {{ input.pdf_file }}\n        {% endif %}\n\n        {% if input.audio_file %}\n        {{ input.audio_file }}\n        {% endif %}\n\n        {% if input.video_file %}\n        {{ input.video_file }}\n        {% endif %}\n\n        {{ ctx.output_format }}\n    "#\n}\n\n// --- Issue Board ---\nclass IssueBoardFacts {\n    board_name string? @description("The name or title of the issue tracking board")\n    sprint_name string? @description("The name of the current sprint or iteration")\n    visible_columns string[] @description("List of column headers visible on the board (e.g. \'To Do\', \'In Progress\')")\n    active_ticket_count int? @description("Count of tickets currently in active/non-terminal states")\n    assignees_visible string[] @description("List of usernames or names of people assigned to tickets")\n}\n\nfunction ExtractIssueBoardFacts(input: EvidenceInput) -> IssueBoardFacts {\n    client "openai/gpt-4o"\n    prompt #"\n        Extract state from this issue tracking board.\n        \n        {{ _.role("user") }}\n        \n        {% if input.text %}\n        {{ input.text }}\n        {% endif %}\n\n        {% if input.img %}\n        {{ input.img }}\n        {% endif %}\n\n        {% if input.pdf_file %}\n        {{ input.pdf_file }}\n        {% endif %}\n\n        {% if input.audio_file %}\n        {{ input.audio_file }}\n        {% endif %}\n\n        {% if input.video_file %}\n        {{ input.video_file }}\n        {% endif %}\n\n        {{ ctx.output_format }}\n    "#\n}\n\n// --- Performance Report ---\nclass PerformanceFacts {\n    tool_name string? @description("Name of the performance testing tool (e.g. JMeter, K6)")\n    performance_score int? @description("Overall performance score if provided (e.g. Lighthouse score)")\n    avg_response_time_ms float? @description("Average response time in milliseconds")\n    p95_response_time_ms float? @description("95th percentile response time in milliseconds")\n    throughput_rps float? @description("Requests per second processed")\n    error_rate_percent float? @description("Percentage of requests that resulted in errors")\n}\n\nfunction ExtractPerformanceFacts(input: EvidenceInput) -> PerformanceFacts {\n    client "openai/gpt-4o"\n    prompt #"\n        Extract performance benchmarks and metrics.\n        \n        {{ _.role("user") }}\n        \n        {% if input.text %}\n        {{ input.text }}\n        {% endif %}\n\n        {% if input.img %}\n        {{ input.img }}\n        {% endif %}\n\n        {% if input.pdf_file %}\n        {{ input.pdf_file }}\n        {% endif %}\n\n        {% if input.audio_file %}\n        {{ input.audio_file }}\n        {% endif %}\n\n        {% if input.video_file %}\n        {{ input.video_file }}\n        {% endif %}\n\n        {{ ctx.output_format }}\n    "#\n}\n',
    "generators.baml": '// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: "python/pydantic", "typescript", "ruby/sorbet", "rest/openapi"\n    output_type "python/pydantic"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir "../src/"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version "0.214.0"\n\n    // Valid values: "sync", "async"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n',
}


def get_baml_files():
    return _file_map
