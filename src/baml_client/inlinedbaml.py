# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

_file_map = {
    "clients.baml": '// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\n// Using the new OpenAI Responses API for enhanced formatting\nclient<llm> CustomGPT5 {\n  provider openai-responses\n  options {\n    model "gpt-5"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT5Mini {\n  provider openai-responses\n  retry_policy Exponential\n  options {\n    model "gpt-5-mini"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\n// Openai with chat completion\nclient<llm> CustomGPT5Chat {\n  provider openai\n  options {\n    model "gpt-5"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\n// Latest Anthropic Claude 4 models\nclient<llm> CustomOpus4 {\n  provider anthropic\n  options {\n    model "claude-opus-4-1-20250805"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\nclient<llm> CustomSonnet4 {\n  provider anthropic\n  options {\n    model "claude-sonnet-4-20250514"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\nclient<llm> CustomHaiku {\n  provider anthropic\n  retry_policy Constant\n  options {\n    model "claude-3-5-haiku-20241022"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n// Example Google AI client (uncomment to use)\n// client<llm> CustomGemini {\n//   provider google-ai\n//   options {\n//     model "gemini-2.5-pro"\n//     api_key env.GOOGLE_API_KEY\n//   }\n// }\n\n// Example AWS Bedrock client (uncomment to use)\n// client<llm> CustomBedrock {\n//   provider aws-bedrock\n//   options {\n//     model "anthropic.claude-sonnet-4-20250514-v1:0"\n//     region "us-east-1"\n//     // AWS credentials are auto-detected from env vars\n//   }\n// }\n\n// Example Azure OpenAI client (uncomment to use)\n// client<llm> CustomAzure {\n//   provider azure-openai\n//   options {\n//     model "gpt-5"\n//     api_key env.AZURE_OPENAI_API_KEY\n//     base_url "https://MY_RESOURCE_NAME.openai.azure.com/openai/deployments/MY_DEPLOYMENT_ID"\n//     api_version "2024-10-01-preview"\n//   }\n// }\n\n// Example Vertex AI client (uncomment to use)\n// client<llm> CustomVertex {\n//   provider vertex-ai\n//   options {\n//     model "gemini-2.5-pro"\n//     location "us-central1"\n//     // Uses Google Cloud Application Default Credentials\n//   }\n// }\n\n// Example Ollama client for local models (uncomment to use)\n// client<llm> CustomOllama {\n//   provider openai-generic\n//   options {\n//     base_url "http://localhost:11434/v1"\n//     model "llama4"\n//     default_role "user" // Most local models prefer the user role\n//     // No API key needed for local Ollama\n//   }\n// }\n\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\nclient<llm> CustomFast {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [CustomGPT5Mini, CustomHaiku]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [CustomGPT5Mini, CustomGPT5]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}',
    "control.baml": '// 1. Define types to distinguish between Merge Gates and Process/Audit checks\nenum RuleType {\n    Blocking @description(#"\n        Rules that must pass immediately before a merge or release.\n        Example: "All tests passed", "Coverage > 80%"\n    "#)\n    Audit @description(#"\n        Maintenance or process rules that are measured over time, not per PR.\n        Example: "Flaky tests fixed within 2 days", "Quarterly policy review"\n    "#)\n}\n\nclass Rule {\n    // Added ID for easier tracking in logs/UI\n    id string @description("Short identifier, e.g. \'TEST-01\'")\n    \n    // Added type to solve the "Process vs Gate" risk\n    type RuleType\n    \n    description string\n    \n    // Updated description to solve "Ambiguity" and "Critical Path" risks\n    logic string @description(#"\n        Propositional Logic. Be explicit with variable values.\n        BAD: "CriticalPaths -> 100% Coverage"\n        GOOD: "FilePath.contains(\'security\', \'payment\') -> Coverage == 100%"\n    "#)\n    \n    exceptions string[] @description(#"\n        List of specific scenarios where this rule can be skipped.\n    "#)\n}\n\nclass Control {\n    name string\n    description string\n    rules Rule[]\n}\n\nfunction ExtractControl(control: string) -> Control {\n    client "openai-responses/gpt-5-mini" \n    prompt #"\n        Extract the control logic from the following content into a structured format.\n\n        Guidelines for Extraction:\n        1. **Atomic Rules**: Do not combine multiple thresholds into one logic string. \n           - IF the text says "Unit coverage 80% and Integration coverage 60%"\n           - THEN create TWO separate rules.\n        2. **Explicit Logic**: If the text defines a term (e.g., "Critical Paths are payment and auth"), use the explicit values (\'payment\', \'auth\') in the logic string, not the abstract term.\n        3. **Process vs Blocking**: Distinguish between rules that stop a release (Blocking) and rules regarding team habits/maintenance (Audit).\n\n        Content:\n        {{ control }}\n\n        {{ ctx.output_format }}\n    "#\n}\n\ntest code_review_control {\n    functions [ExtractControl]\n    args {\n        control #"\n            # Independent Code Review\nChanges are reviewed by an independent code reviewer prior to code being merged and released to production. Independent code review restricts the ability for anyone individual from taking a change production without involving an independent party (4-eyes principle).\n\n## Control Attributes\n* Code Reviews are performed prior to committing a change to the main branch\n* Code Review approvals are performed by independent code reviewers\n* Testing is performed in accordance with the testing policy\n\n# Testing Policy\n\n## Overview\n\nThis policy establishes minimum testing requirements for all code changes to ensure quality, reliability, and maintainability of the codebase.\n\n## Minimum Test Coverage Requirements\n\n### Code Coverage Thresholds\n\n- **Unit Tests**: Minimum 80% code coverage for all new code\n- **Integration Tests**: Minimum 60% coverage for critical integration points\n- **Critical Paths**: 100% coverage required for security-sensitive, payment, and authentication flows\n- **Legacy Code**: Existing code must maintain or improve current coverage levels\n\n### Coverage Metrics\n\nCoverage is measured using:\n- Line coverage (primary metric)\n- Branch coverage (minimum 70%)\n- Function coverage (minimum 80%)\n\nCoverage reports must be generated and reviewed as part of the code review process.\n\n## Pre-Release Test Requirements\n\n### Mandatory Test Execution\n\nAll code must pass the following test suites before release:\n\n1. **Unit Tests**: All unit tests must pass with zero failures\n2. **Integration Tests**: All integration tests must pass\n3. **End-to-End Tests**: Critical user journeys must pass\n4. **Performance Tests**: Performance benchmarks must be met (if applicable)\n5. **Security Tests**: Security scan results must be reviewed and approved\n\n### Test Execution Environment\n\n- Tests must run in CI/CD pipeline before merge\n- All tests must pass in a clean environment (no cached dependencies)\n- Test results must be visible in pull request status checks\n- Flaky tests must be fixed or removed before release\n\n### Pre-Release Checklist\n\nBefore any release, the following must be verified:\n\n- [ ] All automated tests pass\n- [ ] Test coverage meets minimum thresholds\n- [ ] No known critical or high-severity bugs\n- [ ] Performance tests pass (if applicable)\n- [ ] Security tests pass\n- [ ] Manual smoke tests completed for critical features\n\n## Test Quality Standards\n\n### Test Requirements\n\n- Tests must be deterministic and repeatable\n- Tests must be independent and not rely on execution order\n- Tests must clean up after themselves (no side effects)\n- Test names must clearly describe what is being tested\n- Tests must include both positive and negative test cases\n\n### Test Maintenance\n\n- Flaky tests must be fixed within 2 business days or removed\n- Tests that fail must be fixed before code merge\n- Obsolete tests must be removed or updated\n- Test code quality must meet the same standards as production code\n\n## Exceptions and Waivers\n\n### Exception Process\n\nThe following types of changes may be exempt from this policy:\n\n1. **Legacy Code Integration**: Changes that integrate with legacy systems where adding test coverage is technically impractical or would require significant refactoring of untested legacy code\n2. **Third-Party Dependencies**: Changes that only involve configuration or integration with third-party libraries, services, or APIs that cannot be directly tested or mocked\n3. **Proof of Concept / Experimental Features**: Experimental code, prototypes, or proof-of-concept implementations that are explicitly marked as temporary and not intended for production use\n4. **Documentation-Only Changes**: Changes that only modify documentation, comments, README files, or non-executable configuration files\n5. **Build and Deployment Scripts**: Changes to CI/CD pipelines, build scripts, deployment configurations, or infrastructure-as-code that don\'t affect application logic\n6. **Dependency Updates**: Updates to dependencies, package versions, or third-party libraries where the change is limited to version bumps without functional modifications\n7. **Refactoring with No Behavioral Changes**: Code refactoring that maintains exact functional behavior and is verified through existing test coverage\n\n## Review and Updates\n\nThis policy is reviewed quarterly and updated as needed based on:\n- Team feedback\n- Industry best practices\n- Project requirements\n- Tooling improvements\n\n---\n\n**Last Updated**: 25/09/2025\n**Policy Owner**: Engineering Leadership\n**Review Frequency**: Quarterly\n\n        "#\n    }\n}',
    "generators.baml": '// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: "python/pydantic", "typescript", "ruby/sorbet", "rest/openapi"\n    output_type "python/pydantic"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir "../src/"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version "0.214.0"\n\n    // Valid values: "sync", "async"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n',
    "resume.baml": '// Defining a data model.\nclass Resume {\n  name string\n  email string\n  experience string[]\n  skills string[]\n}\n\n// Create a function to extract the resume from a string.\nfunction ExtractResume(resume: string) -> Resume {\n  // Specify a client as provider/model-name\n  // You can also use custom LLM params with a custom client name from clients.baml like "client CustomGPT5" or "client CustomSonnet4"\n  client "openai-responses/gpt-5-mini" // Set OPENAI_API_KEY to use this client.\n  prompt #"\n    Extract from this content:\n    {{ resume }}\n\n    {{ ctx.output_format }}\n  "#\n}\n\n\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest vaibhav_resume {\n  functions [ExtractResume]\n  args {\n    resume #"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    "#\n  }\n}\n',
}


def get_baml_files():
    return _file_map
